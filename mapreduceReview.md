# Map-Reduce

## Overview
1. Hadoop MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.
2. A __*MapReduce job*__ usually splits the input data-set into independent chunks which are processed by the  __*map tasks*__ in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the __*reduce tasks*__. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks.
3. The MapReduce framework consists of a single __*master ResourceManager*__, one __*slave NodeManager*__ per cluster-node, and __*MRAppMaster*__ per application.

## Input & Output 
1. The MapReduce framework operates exclusively on <key, value> pairs, that is, the framework views the input to the job as a set of <key, value> pairs and produces a set of <key, value> pairs as the output of the job, conceivably of different types.
2. The __key__ and __value__ classes have to be serializable by the framework and hence need to implement the __Writable__ interface. Additionally, the key classes have to implement the __WritableComparable__ interface to facilitate sorting by the framework.
3. Input and Output types of a MapReduce job:
(input) <k1, v1> -> *__map__* -> <k2, v2> -> *__combine__* -> <k2, v2> -> *__reduce__* -> <k3, v3> (output)

## MapReduce - User Interfaces

### Mapper
1. Class *Mapper* maps input key/value pairs to a set of intermediate key/value pairs. The Hadoop MapReduce framework spawns one map task for each *__InputSplit__* generated by the *__InputFormat__* for the job. The number of maps is usually driven by the total size of the inputs, that is, the total number of blocks of the input files.
2. *__Mapper__* implementations are passed to the job via __Job.setMapperClass(Class)__ method. The framework then calls __map(WritableComparable, Writable, Context)__ for each key/value pair in the *__InputSplit__* for that task.
3. Application can perform __setup()__ and __cleanup()__. The process will be like: setup -> map -> cleanup. Same thing to recuder, setup -> reduce -> cleanup.
4. Users can optionally specify a *__combiner__*, via __Job.setCombinerClass(Class)__, to perform local aggregation of the intermediate outputs, which helps to cut down the amount of data transferred from the Mapper to the Reducer.
5. Users can control the grouping by specifying a __Comparator__ via __Job.setGroupingComparatorClass(Class)__. The total number of partitions is the same as the number of reduce __tasks__ for the job. 

### Reducer
1. The *__Mapper__* outputs are sorted and then partitioned per *__Reducer__*. The total number of partitions is the same as the number of *__reduce tasks__* for the *__job__*. The number of *__reduces__* for the *__job__* is set by the user via __Job.setNumReduceTasks(int)__. Users can control which keys (and hence records) go to which __Reducer__ by implementing a custom Partitioner.
2. *__Reducer__* implementations are passed the *__Job__* for the job via the __Job.setReducerClass(Class)__ method. The framework then calls __reduce(WritableComparable, Iterable<Writable>, Context)__ method for each <key, (list of values)> pair in the grouped inputs.
3. Three phases: __shuffle, sort and reduce__.
  * 3.1  __Shuffle__: Input to the Reducer is the sorted output of the mappers. In this phase the framework fetches the relevant partition of the output of all the mappers, via HTTP.
  + 3.2.1 __Sort__: The framework groups Reducer inputs by keys (since different mappers may have output the same key) in this stage.The shuffle and sort phases occur simultaneously; while map-outputs are being fetched they are merged.
  + 3.2.2 __Secondary Sort__: If equivalence rules for grouping the intermediate keys are required to be different from those for grouping keys before reduction, then one may specify a Comparator via Job.setSortComparatorClass(Class). Since Job.setGroupingComparatorClass(Class) can be used to control how intermediate keys are grouped, these can be used in conjunction to simulate secondary sort on values.
  - 3.3  __Reduce__: In this phase the __reduce(WritableComparable, Iterable<Writable>, Context)__ method is called for each <key, (list of values)> pair in the grouped inputs. The unsorted output of the reduce task is typically written to the FileSystem via __Context.write(WritableComparable, Writable)__.
4. The number of reduces: The right number of reduces seems to be 0.95 or 1.75 multiplied by (<no. of *__nodes__*> * <no. of maximum *__containers__* per *__nodes__*>). Increasing the number of reduces increases the framework overhead, but increases load balancing and lowers the cost of failures.
5. NONE Reducer: It is legal to set the number of reduce-tasks to zero if no reduction is desired. In this case the outputs of the map-tasks go directly to the FileSystem, into the output path set by __FileOutputFormat.setOutputPath(Job, Path)__. The framework does not sort the map-outputs before writing them out to the FileSystem.

### Partitioner
*__Partitioner__*, by default __HashPartitioner__, partitions the key space. It controls the partitioning of the keys of the intermediate map-outputs. The key (or a subset of the key) is used to derive the partition, typically by a hash function. The total number of partitions is the same as the number of reduce tasks for the job. Hence this controls which of the m reduce tasks the intermediate key (and hence the record) is sent to for reduction.

### Counter
Counter is a facility for MapReduce applications to report its statistics. Mapper and Reducer implementations can use the Counter to report statistics. Hadoop MapReduce comes bundled with a library of generally useful mappers, reducers, and partitioners.

### Job
*__Job__* represents a MapReduce job configuration. Job is typically used to specify the Mapper, combiner (if any), Partitioner, Reducer, InputFormat, OutputFormat implementations.

Reference:
  * https://hadoop.apache.org/docs/r2.8.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#InputSplit
  * https://stackoverflow.com/questions/25432598/what-is-the-mapper-of-reducer-setup-used-for
  * http://www.tomsitpro.com/articles/hadoop-2-vs-1,2-718.html
